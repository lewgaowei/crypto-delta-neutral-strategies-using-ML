{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9134b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timkf\\AppData\\Local\\Temp\\ipykernel_13336\\4288293377.py:41: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  funding = pd.read_csv(\n",
      "C:\\Users\\timkf\\AppData\\Local\\Temp\\ipykernel_13336\\4288293377.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 2828, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 0.000022\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 1.85675e-05\tvalid_1's rmse: 9.53269e-06\n",
      "[200]\ttraining's rmse: 1.36524e-05\tvalid_1's rmse: 9.47307e-06\n",
      "Early stopping, best iteration is:\n",
      "[154]\ttraining's rmse: 1.54581e-05\tvalid_1's rmse: 9.39264e-06\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 294\u001b[39m\n\u001b[32m    291\u001b[39m lstm = LSTM(\u001b[32m128\u001b[39m, return_sequences=\u001b[38;5;28;01mTrue\u001b[39;00m)(inp)\n\u001b[32m    293\u001b[39m att_logits = Dense(\u001b[32m1\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mtanh\u001b[39m\u001b[33m\"\u001b[39m)(lstm)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m att_logits = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m spikes = tf.abs(inp[:,:,SPIKE_IDX])\n\u001b[32m    297\u001b[39m boost = \u001b[32m1\u001b[39m + ATTN_SPIKE_ALPHA * spikes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timkf\\anaconda3\\envs\\QF634_project\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[39m, in \u001b[36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     87\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timkf\\anaconda3\\envs\\QF634_project\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\timkf\\anaconda3\\envs\\QF634_project\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:194\u001b[39m, in \u001b[36mKerasTensor.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mused when constructing Keras Functional models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand `keras.ops`). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    212\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    214\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Multiply, Permute\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# ==========================================================\n",
    "# 0. REPRODUCIBILITY\n",
    "# ==========================================================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# ==========================================================\n",
    "# 1. CONFIG\n",
    "# ==========================================================\n",
    "WINDOW = 48                 # number of 4h windows for the sequence\n",
    "FUTURE_PERIODS = 1          # predict next 4h funding\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 60\n",
    "\n",
    "# Spike handling hyperparams\n",
    "SPIKE_FACTOR_LGBM = 5000.0  \n",
    "SPIKE_WEIGHT_CLIP = 50.0\n",
    "SPIKE_LOSS_GAMMA = 40.0\n",
    "ATTN_SPIKE_ALPHA = 40.0\n",
    "\n",
    "# ==========================================================\n",
    "# 2. LOAD FUNDING + 5-MIN SPOT/FUTURES DATA\n",
    "# ==========================================================\n",
    "funding = pd.read_csv(\n",
    "    r\"D:\\Homework\\QF634\\project\\TAOUSDT_funding_rate_20200101_20251130.csv\",\n",
    "    parse_dates=[\"fundingDateTime\"],\n",
    "    date_parser=lambda x: pd.to_datetime(x, format=\"%Y-%m-%d %H:%M:%S.%f\"),\n",
    ")\n",
    "funding = funding.sort_values(\"fundingDateTime\")\n",
    "funding = funding.rename(columns={\"fundingDateTime\": \"timestamp\", \"fundingRate\": \"funding_rate\"})\n",
    "funding = funding.drop(columns=[\"symbol\", \"formattedFundingDateTime\"])\n",
    "funding.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "spot = pd.read_json(\n",
    "    r\"D:\\Homework\\QF634\\project\\Data\\raw_historical_price\\TAOUSDT_5m_binance_spot_historical_data.json\",\n",
    "    lines=True,\n",
    ").sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "future = pd.read_json(\n",
    "    r\"D:\\Homework\\QF634\\project\\Data\\raw_historical_price\\TAOUSDT_5m_binance_futures_historical_data.json\",\n",
    "    lines=True,\n",
    ").sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "spot.index = pd.to_datetime(spot.index)\n",
    "future.index = pd.to_datetime(future.index)\n",
    "funding.index = pd.to_datetime(funding.index)\n",
    "\n",
    "# ==========================================================\n",
    "# 3. REMOVE LAST 15 MINUTES BEFORE EACH FUNDING TIMESTAMP\n",
    "# ==========================================================\n",
    "fund_times = funding.index.sort_values()\n",
    "\n",
    "clean_spot_windows = {}\n",
    "clean_future_windows = {}\n",
    "\n",
    "for i in range(1, len(fund_times)):\n",
    "    t_prev = fund_times[i - 1]\n",
    "    t_curr = fund_times[i]\n",
    "\n",
    "    window_start = t_prev\n",
    "    window_end = t_curr - pd.Timedelta(minutes=15)  # remove final 15m\n",
    "\n",
    "    spot_window = spot.loc[(spot.index >= window_start) & (spot.index < window_end)]\n",
    "    fut_window = future.loc[(future.index >= window_start) & (future.index < window_end)]\n",
    "\n",
    "    clean_spot_windows[t_curr] = spot_window\n",
    "    clean_future_windows[t_curr] = fut_window\n",
    "\n",
    "# ==========================================================\n",
    "# 4. MANUAL RESAMPLING (OHLCV)\n",
    "# ==========================================================\n",
    "spot_rows = []\n",
    "future_rows = []\n",
    "\n",
    "for t in fund_times[1:]:  # skip first because no full window\n",
    "    sw = clean_spot_windows[t]\n",
    "    fw = clean_future_windows[t]\n",
    "\n",
    "    # ---- Spot ----\n",
    "    spot_rows.append(pd.Series({\n",
    "        \"spot_open\": sw[\"open\"].iloc[0] if len(sw) else np.nan,\n",
    "        \"spot_high\": sw[\"high\"].max() if len(sw) else np.nan,\n",
    "        \"spot_low\": sw[\"low\"].min() if len(sw) else np.nan,\n",
    "        \"spot_close\": sw[\"close\"].iloc[-1] if len(sw) else np.nan,\n",
    "        \"spot_volume\": sw[\"volume\"].sum() if len(sw) else 0.0,\n",
    "        \"spot_quote_volume\": sw[\"quote_asset_volume\"].sum() if len(sw) else 0.0,\n",
    "        \"spot_trades\": sw[\"number_of_trades\"].sum() if len(sw) else 0.0,\n",
    "        \"spot_taker_buy_base\": sw[\"taker_buy_base_asset_volume\"].sum() if len(sw) else 0.0,\n",
    "        \"spot_taker_buy_quote\": sw[\"taker_buy_quote_asset_volume\"].sum() if len(sw) else 0.0,\n",
    "    }, name=t))\n",
    "\n",
    "    # ---- Future ----\n",
    "    future_rows.append(pd.Series({\n",
    "        \"fut_open\": fw[\"open\"].iloc[0] if len(fw) else np.nan,\n",
    "        \"fut_high\": fw[\"high\"].max() if len(fw) else np.nan,\n",
    "        \"fut_low\": fw[\"low\"].min() if len(fw) else np.nan,\n",
    "        \"fut_close\": fw[\"close\"].iloc[-1] if len(fw) else np.nan,\n",
    "        \"fut_volume\": fw[\"volume\"].sum() if len(fw) else 0.0,\n",
    "        \"fut_quote_volume\": fw[\"quote_asset_volume\"].sum() if len(fw) else 0.0,\n",
    "        \"fut_trades\": fw[\"number_of_trades\"].sum() if len(fw) else 0.0,\n",
    "        \"fut_taker_buy_base\": fw[\"taker_buy_base_asset_volume\"].sum() if len(fw) else 0.0,\n",
    "        \"fut_taker_buy_quote\": fw[\"taker_buy_quote_asset_volume\"].sum() if len(fw) else 0.0,\n",
    "    }, name=t))\n",
    "\n",
    "spot_8h = pd.DataFrame(spot_rows)\n",
    "future_8h = pd.DataFrame(future_rows)\n",
    "\n",
    "# ==========================================================\n",
    "# 5. MERGE ALL 4H DATA\n",
    "# ==========================================================\n",
    "df = funding.merge(spot_8h, left_index=True, right_index=True, how=\"left\") \\\n",
    "            .merge(future_8h, left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "# ==========================================================\n",
    "# 6. FEATURE ENGINEERING\n",
    "# ==========================================================\n",
    "def pct_change(series, periods=1):\n",
    "    return series.pct_change(periods)\n",
    "\n",
    "def max_drawdown(arr):\n",
    "    peak = np.maximum.accumulate(arr)\n",
    "    dd = (arr - peak) / peak\n",
    "    return dd.min()\n",
    "\n",
    "df[\"spot_ret_1\"] = pct_change(df[\"spot_close\"])\n",
    "df[\"spot_vol_24\"] = df[\"spot_ret_1\"].rolling(24).std()\n",
    "df[\"spot_mom_24\"] = df[\"spot_close\"] / df[\"spot_close\"].shift(24) - 1\n",
    "df[\"spot_maxdd_120\"] = df[\"spot_close\"].rolling(120).apply(lambda x: max_drawdown(np.array(x)), raw=False)\n",
    "\n",
    "df[\"fut_ret_1\"] = pct_change(df[\"fut_close\"])\n",
    "df[\"fut_vol_24\"] = df[\"fut_ret_1\"].rolling(24).std()\n",
    "df[\"fut_mom_24\"] = df[\"fut_close\"] / df[\"fut_close\"].shift(24) - 1\n",
    "df[\"fut_maxdd_120\"] = df[\"fut_close\"].rolling(120).apply(lambda x: max_drawdown(np.array(x)), raw=False)\n",
    "\n",
    "df[\"spread_close\"] = df[\"fut_close\"] - df[\"spot_close\"]\n",
    "df[\"spread_ret_1\"] = df[\"spread_close\"].pct_change()\n",
    "\n",
    "df[\"funding_delta_1\"] = df[\"funding_rate\"].diff()\n",
    "df[\"funding_vol_24\"] = df[\"funding_rate\"].pct_change().rolling(24).std()\n",
    "df[\"funding_mom_24\"] = df[\"funding_rate\"] / df[\"funding_rate\"].shift(24) - 1\n",
    "\n",
    "df[\"funding_slope_6\"] = df[\"funding_rate\"].rolling(6).apply(lambda x: np.polyfit(np.arange(6), x, 1)[0])\n",
    "df[\"funding_mean_6\"]  = df[\"funding_rate\"].rolling(6).mean()\n",
    "\n",
    "df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# ==========================================================\n",
    "# 7. BUILD TARGET & SEQUENCES\n",
    "# ==========================================================\n",
    "y_multi = np.array([\n",
    "    df[\"funding_rate\"].iloc[i+WINDOW:i+WINDOW+FUTURE_PERIODS].values\n",
    "    for i in range(len(df) - WINDOW - FUTURE_PERIODS)\n",
    "], dtype=np.float32)\n",
    "\n",
    "feature_cols = [\n",
    "    \"spot_close\", \"spot_ret_1\", \"spot_vol_24\", \"spot_mom_24\", \"spot_maxdd_120\",\n",
    "    \"fut_close\", \"fut_ret_1\", \"fut_vol_24\", \"fut_mom_24\", \"fut_maxdd_120\",\n",
    "    \"spread_close\", \"spread_ret_1\",\n",
    "    \"funding_delta_1\", \"funding_vol_24\", \"funding_mom_24\",\n",
    "    \"funding_slope_6\", \"funding_mean_6\",\n",
    "]\n",
    "\n",
    "seq_X = []\n",
    "tab_X = []\n",
    "for i in range(len(df) - WINDOW - FUTURE_PERIODS):\n",
    "    seq_X.append(df.iloc[i:i+WINDOW][feature_cols].values)\n",
    "    tab_X.append(df.iloc[i+WINDOW][feature_cols].values)\n",
    "\n",
    "seq_X = np.array(seq_X, dtype=np.float32)\n",
    "tab_X = np.array(tab_X, dtype=np.float32)\n",
    "\n",
    "# ==========================================================\n",
    "# 8. SPIKE WEIGHTS\n",
    "# ==========================================================\n",
    "weights = []\n",
    "fd = df[\"funding_delta_1\"].values\n",
    "\n",
    "for i in range(len(df) - WINDOW - FUTURE_PERIODS):\n",
    "    w = np.max(np.abs(fd[i:i+WINDOW]))\n",
    "    w = 1 + SPIKE_FACTOR_LGBM * w\n",
    "    w = min(w, SPIKE_WEIGHT_CLIP)\n",
    "    weights.append(w)\n",
    "\n",
    "weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "# ==========================================================\n",
    "# 9. TRAIN / VAL / TEST SPLITS\n",
    "# ==========================================================\n",
    "N = len(seq_X)\n",
    "train_end = int(0.8*N)\n",
    "val_end = int(0.9*N)\n",
    "\n",
    "X_seq_train, X_seq_val, X_seq_test = seq_X[:train_end], seq_X[train_end:val_end], seq_X[val_end:]\n",
    "X_tab_train, X_tab_val, X_tab_test = tab_X[:train_end], tab_X[train_end:val_end], tab_X[val_end:]\n",
    "y_train, y_val, y_test = y_multi[:train_end], y_multi[train_end:val_end], y_multi[val_end:]\n",
    "w_train, w_val, w_test = weights[:train_end], weights[train_end:val_end], weights[val_end:]\n",
    "\n",
    "# ==========================================================\n",
    "# 10. SCALING\n",
    "# ==========================================================\n",
    "tab_scaler = StandardScaler()\n",
    "X_tab_train_s = tab_scaler.fit_transform(X_tab_train)\n",
    "X_tab_val_s = tab_scaler.transform(X_tab_val)\n",
    "X_tab_test_s = tab_scaler.transform(X_tab_test)\n",
    "\n",
    "seq_scaler = StandardScaler().fit(X_seq_train.reshape(-1, seq_X.shape[2]))\n",
    "\n",
    "def scale_seq(x):\n",
    "    x2 = x.reshape(-1, x.shape[2])\n",
    "    x2 = seq_scaler.transform(x2)\n",
    "    return x2.reshape(x.shape[0], x.shape[1], x.shape[2])\n",
    "\n",
    "X_seq_train_s = scale_seq(X_seq_train)\n",
    "X_seq_val_s = scale_seq(X_seq_val)\n",
    "X_seq_test_s = scale_seq(X_seq_test)\n",
    "\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "y_train_s = y_scaler.transform(y_train)\n",
    "y_val_s = y_scaler.transform(y_val)\n",
    "\n",
    "# LGB part (sum target)\n",
    "y_train_sum = y_train.sum(axis=1)\n",
    "y_val_sum = y_val.sum(axis=1)\n",
    "y_test_sum = y_test.sum(axis=1)\n",
    "\n",
    "# ==========================================================\n",
    "# 11. TRAIN LGBM\n",
    "# ==========================================================\n",
    "lgb_train = lgb.Dataset(X_tab_train_s, label=y_train_sum, weight=w_train)\n",
    "lgb_val = lgb.Dataset(X_tab_val_s, label=y_val_sum, weight=w_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\":\"regression\",\n",
    "    \"metric\":\"rmse\",\n",
    "    \"learning_rate\":0.04,\n",
    "    \"num_leaves\":64,\n",
    "    \"seed\":SEED,\n",
    "}\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    params, lgb_train,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(50),\n",
    "        lgb.log_evaluation(100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pred_lgb_train = lgb_model.predict(X_tab_train_s)\n",
    "pred_lgb_val = lgb_model.predict(X_tab_val_s)\n",
    "pred_lgb_test = lgb_model.predict(X_tab_test_s)\n",
    "\n",
    "# ==========================================================\n",
    "# 12. RESIDUAL TARGET FOR RNN\n",
    "# ==========================================================\n",
    "lgb_train_scaled = y_scaler.transform(pred_lgb_train.reshape(-1,1))\n",
    "lgb_val_scaled   = y_scaler.transform(pred_lgb_val.reshape(-1,1))\n",
    "\n",
    "res_train = y_train_s - lgb_train_scaled\n",
    "res_val = y_val_s - lgb_val_scaled\n",
    "\n",
    "# ==========================================================\n",
    "# 13. SPIKE-AWARE ATTENTION LSTM\n",
    "# ==========================================================\n",
    "SPIKE_IDX = feature_cols.index(\"funding_delta_1\")\n",
    "\n",
    "def spike_loss(y_true, y_pred):\n",
    "    w = 1 + SPIKE_LOSS_GAMMA * tf.abs(y_true)\n",
    "    return tf.reduce_mean(w * tf.square(y_true - y_pred))\n",
    "\n",
    "\n",
    "inp = Input(shape=(WINDOW, seq_X.shape[2]))\n",
    "\n",
    "# ---- LSTM ----\n",
    "lstm = LSTM(128, return_sequences=True)(inp)\n",
    "\n",
    "# ---- Attention scores ----\n",
    "att_logits = Dense(1, activation=\"tanh\")(lstm)                     # (B, T, 1)\n",
    "att_logits = Lambda(lambda x: tf.squeeze(x, axis=-1))(att_logits) # (B, T)\n",
    "\n",
    "# ---- Extract spike magnitude ----\n",
    "spikes = Lambda(lambda x: tf.abs(x[:, :, SPIKE_IDX]))(inp)        # (B, T)\n",
    "\n",
    "# ---- Boost attention by spike magnitude ----\n",
    "boost = Lambda(lambda s: 1.0 + ATTN_SPIKE_ALPHA * s)(spikes)      # (B, T)\n",
    "att_logits = Lambda(lambda ab: ab[0] * ab[1])([att_logits, boost])\n",
    "\n",
    "# ---- Softmax attention ----\n",
    "att = tf.keras.layers.Activation(\"softmax\")(att_logits)           # (B, T)\n",
    "\n",
    "# ---- Expand attention to match LSTM hidden dim ----\n",
    "att_exp = RepeatVector(128)(att)                                  # (B, 128, T)\n",
    "att_exp = Permute([2,1])(att_exp)                                 # (B, T, 128)\n",
    "\n",
    "# ---- Weighted sum context ----\n",
    "context = Multiply()([lstm, att_exp])                             # (B, T, 128)\n",
    "context = Lambda(lambda x: tf.reduce_sum(x, axis=1))(context)     # (B, 128)\n",
    "\n",
    "# ---- Dense layers ----\n",
    "dense = Dense(64, activation=\"relu\")(context)\n",
    "dense = Dropout(0.2)(dense)\n",
    "out = Dense(1)(dense)\n",
    "\n",
    "# ---- Build model ----\n",
    "rnn_model = Model(inp, out)\n",
    "rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=spike_loss)\n",
    "\n",
    "rnn_model.fit(\n",
    "    X_seq_train_s, res_train,\n",
    "    validation_data=(X_seq_val_s, res_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[EarlyStopping(10, restore_best_weights=True),\n",
    "               ReduceLROnPlateau(5, factor=0.5)],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# 14. HYBRID PREDICTION (CORRECTED)\n",
    "# ==========================================================\n",
    "def hybrid_predict(X_seq, pred_lgb):\n",
    "    pred_lgb_scaled = y_scaler.transform(pred_lgb.reshape(-1,1))\n",
    "    pred_res_scaled = rnn_model.predict(X_seq)\n",
    "    y_scaled = pred_lgb_scaled + pred_res_scaled\n",
    "    y = y_scaler.inverse_transform(y_scaled)\n",
    "    return y.ravel()\n",
    "\n",
    "pred_hybrid = hybrid_predict(X_seq_test_s, pred_lgb_test)\n",
    "\n",
    "# ==========================================================\n",
    "# 15. EVALUATION\n",
    "# ==========================================================\n",
    "def metrics(true, pred, name):\n",
    "    rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    print(f\"{name} RMSE={rmse:.8f}, MAE={mae:.8f}\")\n",
    "\n",
    "metrics(y_test_sum, pred_lgb_test, \"LGBM\")\n",
    "metrics(y_test_sum, pred_hybrid, \"Hybrid RNN+LGBM\")\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(y_test_sum, label=\"Actual\", color=\"black\")\n",
    "plt.plot(pred_lgb_test, label=\"LGBM\", alpha=0.7)\n",
    "plt.plot(pred_hybrid, label=\"Hybrid\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# 16. SAVE MODELS & SCALERS\n",
    "# ==========================================================\n",
    "lgb_model.save_model(\"lgb_model.txt\")\n",
    "rnn_model.save(\"rnn_attention_spike_model.h5\")\n",
    "joblib.dump(tab_scaler, \"tab_scaler.pkl\")\n",
    "joblib.dump(seq_scaler, \"seq_scaler.pkl\")\n",
    "joblib.dump(y_scaler, \"y_scaler.pkl\")\n",
    "\n",
    "print(\"All models saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QF634_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
